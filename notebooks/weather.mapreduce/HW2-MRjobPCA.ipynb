{
 "metadata": {
  "name": "",
  "signature": "sha256:21a7ac6fc7f798ed8d21dd03016c3ba208b6fceddf58dce014ee825ba3676b39"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Part2: Perform PCA on MapReduce\n",
      "This notebook takes the result from part 1 as input and perform PCA analysis on a MapReduce framework. Specially, we write a two-step MapReduce for this purpose.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "import sys,os\n",
      "\n",
      "cwd=os.getcwd()\n",
      "path=cwd.split('/')\n",
      "home_dir='/'.join(path[:-2])\n",
      "print home_dir\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "print Creds.keys()\n",
      "print Creds['mrjob'].keys()\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "print ID\n",
      "\n",
      "job_flow_id=find_waiting_flow(key_id,secret_key)\n",
      "print job_flow_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['launcher', 'mrjob']\n",
        "['key_id', 'secret_key', 's3_logs', 'ID', 's3_scratch']\n",
        "x7jin\n",
        "<boto.emr.emrobject.JobFlow object at 0x3532cd0>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " no_script.yoavfreund.20140525.174308.746673 j-LTOJMJ14G840 WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x38a0110> no_script.yoavfreund.20140525.194233.003342 j-2KCJE554SGITB WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x3b9cb10> no_script.yoavfreund.20140526.014348.966938 j-6T8VIKMY8RHX WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x3d8b890> no_script.yoavfreund.20140526.014403.503214 j-31UKS93V80CN7 WAITING\n",
        "j-31UKS93V80CN7\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dir=home_dir+'/data/weather'\n",
      "print data_dir\n",
      "%cd $data_dir"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/home/ubuntu/UCSD_BigData/data/weather\n",
        "/home/ubuntu/UCSD_BigData/data/weather\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step 1: Data preparation\n",
      "In this step MRjob filter out the invalid data as part 1 defines and generates the data form for step 2.\n",
      "\n",
      "Input: station, measurement, year, d1,....d365 <br />\n",
      "output: index(partition), TMAX[365] + TMIN[365] <br />\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Step2: Perform PCA through MapReduce###\n",
      "In this step MRjob get all the measurements with dimension 730 from each partition. It computes the cov matrix, eigenvectors, and output the number of eigenvectors that can explain >98% variance.\n",
      "\n",
      "####Compute the covariance matrix (for each partition)\n",
      "Covariance matrix can be computed as \n",
      "$\\sum_{} = \\frac{1}{m} \\sum_{i=1}^m (X_i X_i^T) - \\mu \\mu^T$. The mean vector $\\mu$ can be computed as $\\mu = \\frac{1}{m} \\sum_{i=1}^m x_i$.\n",
      "Therefore the summation form of the above equations can be mapped to several mappers and then reducer just sum up the all the results from mappers to produce the final matrix. In order to relieve the load of reducers, we use combiners to sum up partial results.\n",
      "\n",
      "####Compute eigenvectors (for each partition)\n",
      "Reducer is in charge of computing the eigenvectos. We direclty apply\n",
      "U,D,V=np.linalg.svd(cov) on the covariance matrix.\n",
      "\n",
      "####Time complexity analysis\n",
      "Here we do a simple algorithm time complexity analysis. Suppose we have $m$ training examples and the data dimension is $n$, and there are $P$ mappers. To simplify the scenario, we assume a signle core model for each physical node. On a single node, the time complexity of covariance computations and eigenvecotr computation are $O(mn^2)$ and $O(n^3)$ separately, so the total time complexity is: \n",
      "$$O(mn^2 + n^3)$$ \n",
      "On mapreduce framework, the theoretical time complexity is: $$O(\\frac{mn^2}{P} + n^3)$$ \n",
      "Since $n>>m$ in our case, the ideal speedup that could be obtained by using MapReduce is $P$. \n",
      "\n",
      "In practice, the disk and network I/O communication overhead defintely cannot be overlooked. So an intuitive hypothesis based on the analysis is that it may not be a good idea to perform mapreduce when $P$ is small. \n",
      "\n",
      "(Xinxin's note: possible to look at the time overhead of the disk and network I/O on EMR? possible to build a model to estimate the real-world time of a machine learning algorithm on MapReduce? how to make the algorithm more efficient considering I/O ?) \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile MrjobPCA.py\n",
      "#!/usr/bin/python\n",
      "import re,pickle,base64,zlib\n",
      "from sys import stderr\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages') # a hack because anaconda made mrjob unreachable\n",
      "from mrjob.job import MRJob\n",
      "from mrjob.protocol import *\n",
      "import traceback\n",
      "from functools import wraps\n",
      "from sys import stderr\n",
      "import numpy as np\n",
      "import csv\n",
      "import pandas as pd\n",
      "from mrjob.step import MRStep\n",
      "\n",
      "\n",
      "#Merge weather raw data with its node index\n",
      "class MrjobPCA(MRJob):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        super(MrjobPCA, self).__init__(*args, **kwargs)\n",
      "        self.partitions = {}\n",
      "    \n",
      "    def configure_options(self):\n",
      "        super(MrjobPCA,self).configure_options()\n",
      "        self.add_file_option('--partitions')\n",
      "        #This loads the lookup file into a field on the object\n",
      "    \n",
      "    def check_integrity(self,meas,year,days,num_defined):\n",
      "        year=int(year)\n",
      "        if year<1000 or year > 2014: return False\n",
      "        if meas=='': return False\n",
      "        if len(days) != 365: return False\n",
      "        if num_defined < 188: return False\n",
      "        return True  \n",
      "     \n",
      "    def mapper_filter(self, _, line):  # should output (station, year) (data of TMAX or data of TMIN)\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            station = elements[0]\n",
      "            meas = elements[1]\n",
      "            year = elements[2]\n",
      "            days = elements[3:]\n",
      "            num_defined = sum([e != '' for e in days])\n",
      "            if self.check_integrity(meas,year,days, num_defined)==True and (meas == 'TMAX' or meas == 'TMIN'):\n",
      "                yield (station, year), days\n",
      "        except Exception as e: \n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            pass\n",
      "        \n",
      "    def index_init(self):\n",
      "        f= open(self.options.partitions,'rb')\n",
      "        reader = csv.reader(f)\n",
      "        for line in reader:\n",
      "            try:\n",
      "                station = line[0]\n",
      "                index = int(line[1])\n",
      "                self.partitions[station] = index\n",
      "            except Exception as e: \n",
      "                pass\n",
      "        f.close()\n",
      "\n",
      "    def reducer_filter(self,key,vectors):   \n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','reducer-all',1)\n",
      "            index = self.partitions[key[0]]\n",
      "            v = list(vectors)\n",
      "            if len(v) == 2:\n",
      "                data = v[0] + v[1]\n",
      "                yield(index, data)\n",
      "                #yield (key[0],data)\n",
      "        except Exception as e: \n",
      "            self.increment_counter('MrJob Counters','reducer-error',1)    \n",
      "            #stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            #traceback.print_exc(file=stderr) \n",
      "            pass\n",
      "    \n",
      "    \n",
      "    def reducer_pca(self, index, value):\n",
      "        try:\n",
      "            data = pd.DataFrame(value)\n",
      "            data[data == ''] = float('NaN')\n",
      "            data = data.astype(float)\n",
      "            M = data.transpose()\n",
      "            M=M.dropna(axis=1)\n",
      "            (columns,rows)= np.shape(M)\n",
      "            Mean=np.mean(M, axis=1).values\n",
      "            C=np.zeros([columns,columns])   # Sum\n",
      "            N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
      "            for i in range(rows):\n",
      "                row=M.iloc[:,i]-Mean;\n",
      "                outer=np.outer(row,row)\n",
      "                valid=np.isnan(outer)==False\n",
      "                C[valid]=C[valid]+outer[valid]  \n",
      "                N[valid]=N[valid]+1\n",
      "            valid_outer=np.multiply(1-np.isnan(N),N>0)\n",
      "            cov=np.divide(C,N)\n",
      "            U,D,V=np.linalg.svd(cov)\n",
      "            cum_sum = np.cumsum(D[:])/np.sum(D)\n",
      "            k = columns\n",
      "            for k in range(columns):\n",
      "                if cum_sum[k] >= 0.98:\n",
      "                    break \n",
      "            yield (index,k)\n",
      "        except Exception as e: \n",
      "            self.increment_counter('MrJob Counters','reducer-error',1)\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "            pass\n",
      "        \n",
      "    def steps(self):\n",
      "        return [\n",
      "            MRStep(mapper=self.mapper_filter,\n",
      "                   reducer_init = self.index_init,\n",
      "                   reducer=self.reducer_filter),\n",
      "            MRStep(reducer=self.reducer_pca)\n",
      "        ]\n",
      "\n",
      "        \n",
      "if __name__ == '__main__':\n",
      "    MrjobPCA.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Test small dataset first."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python MrjobPCA.py -r emr --emr-job-flow-id $job_flow_id $data_dir/ALL.small.csv --partitions ./index.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/MrjobPCA.ubuntu.20140528.012106.544483\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://xinxin.bucket/scratch/MrjobPCA.ubuntu.20140528.012106.544483/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-31UKS93V80CN7\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 31.0s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012106.544483: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 62.1s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012106.544483: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 93.2s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012106.544483: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 124.2s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012106.544483: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 155.3s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012106.544483: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 139.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters may not have been uploaded to S3 yet. Try again in 5 minutes with: mrjob fetch-logs --counters j-31UKS93V80CN7\r\n",
        "Counters from step 1:\r\n",
        "  (no counters found)\r\n",
        "Counters from step 2:\r\n",
        "  (no counters found)\r\n",
        "Streaming final output from s3://xinxin.bucket/scratch/MrjobPCA.ubuntu.20140528.012106.544483/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "121 14\t\r\n",
        "removing tmp directory /tmp/MrjobPCA.ubuntu.20140528.012106.544483\r\n",
        "Removing all files in s3://xinxin.bucket/scratch/MrjobPCA.ubuntu.20140528.012106.544483/\r\n"
       ]
      }
     ],
     "prompt_number": 270
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python MrjobPCA.py -r emr --emr-job-flow-id $job_flow_id  hdfs:/weather/weather.csv --partitions ./index.csv > pca_results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/MrjobPCA.ubuntu.20140528.012532.635218\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://xinxin.bucket/scratch/MrjobPCA.ubuntu.20140528.012532.635218/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-31UKS93V80CN7\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 31.4s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 62.4s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 93.6s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 124.8s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 155.8s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 186.9s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 218.0s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 249.0s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 1 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 280.0s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 311.2s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 342.2s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 373.3s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 404.3s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 435.5s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 466.6s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 497.6s ago, status RUNNING: Running step (MrjobPCA.ubuntu.20140528.012532.635218: Step 2 of 2)\r\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}